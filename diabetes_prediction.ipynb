# 1. Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# 2. Load dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
        'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
df = pd.read_csv(url, header=None, names=cols)

# 3. Explore the data
print(df.head())
print(df.describe())
print(df['Outcome'].value_counts())

# 4. Split features and labels
X = df.drop('Outcome', axis=1) # Define feature matrix X by dropping the target column 'Outcome' from the dataset
y = df['Outcome']

# 5. Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Split the dataset into training and testing sets with 80% training and 20% testing

# 6. Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test) # Apply the same scaling to the test data using the fitted scaler

# 7. Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42) # Initialize a Random Forest classifier with 100 trees and a fixed random seed
model.fit(X_train, y_train)

# 8. Make predictions
y_pred = model.predict(X_test)

# 9. Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred)) # Print the confusion matrix to evaluate the classification performance
print("\nClassification Report:\n", classification_report(y_test, y_pred)) # Print the classification report showing precision, recall, f1-score, and support
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# 10. Feature Importance Plot
importances = model.feature_importances_
feat_names = X.columns
sns.barplot(x=importances, y=feat_names) # Plot feature importances as a horizontal bar chart using seaborn
plt.title("Feature Importances")
plt.show()
